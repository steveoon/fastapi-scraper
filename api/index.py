from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Optional
from scrapegraphai.graphs import SmartScraperMultiGraph
import asyncio
import json
from aiomultiprocess import Pool
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()
openai_key = os.getenv("OPENAI_APIKEY")
openai_real_key = os.getenv("OPENAI_REAL_APIKEY")

app = FastAPI()

prompt = """è¯·ä»ç»™å®šçš„URLä¸­æŠ“å–å†…å®¹ï¼Œå¹¶ç»„ç»‡æˆç»“æ„åŒ–çš„JSONæ ¼å¼ã€‚JSONåº”åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

1. **Title**: é¡µé¢çš„ä¸»æ ‡é¢˜æˆ–å¤´æ¡ã€‚
2. **Description**: å†…å®¹çš„ç®€è¦æ‘˜è¦æˆ–æè¿°ã€‚
3. **Date**: å†…å®¹å‘å¸ƒæˆ–æœ€åæ›´æ–°çš„æ—¥æœŸã€‚
4. **Author**: å‘å¸ƒå†…å®¹çš„ä½œè€…æˆ–ç»„ç»‡åç§°ã€‚
5. **Content**: ä¸»è¦å†…å®¹ï¼ŒæŒ‰æ®µè½æˆ–éƒ¨åˆ†ç»„ç»‡ã€‚
6. **Tags**: æè¿°å†…å®¹çš„ç›¸å…³æ ‡ç­¾æˆ–ç±»åˆ«ã€‚
7. **URL**: æŠ“å–å†…å®¹çš„URLã€‚

è¯·ç¡®ä¿ä¿¡æ¯å‡†ç¡®åˆ†ç±»ï¼Œå†…å®¹æœ‰ä»·å€¼ä¸”ç›¸å…³ã€‚å¦‚æŸå­—æ®µç¼ºå¤±ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚è¾“å‡ºåº”ä¸ºJSONæ ¼å¼å¯¹è±¡åˆ—è¡¨ï¼Œæ¯ä¸ªå¯¹è±¡å¯¹åº”çš„URLåº”æ˜¯å…¶çˆ¬å–å†…å®¹é¡µé¢çš„URLã€‚

è¯·å°½é‡ç²¾ç®€å›å¤å†…å®¹ï¼Œæ§åˆ¶å­—æ•°åœ¨1000ä»¥å†…ï¼Œç¡®ä¿æ¯ä¸ªå­—æ®µçš„å†…å®¹ç®€æ˜æ‰¼è¦ã€‚

ç¤ºä¾‹JSONæ ¼å¼ï¼š

[
    {
        "title": "ç¤ºä¾‹æ ‡é¢˜",
        "description": "è¿™æ˜¯å†…å®¹çš„ç®€è¦æè¿°ã€‚",
        "date": "2024-06-24",
        "author": "å¼ ä¸‰",
        "content": "è¿™æ˜¯ä¸»è¦å†…å®¹ï¼ŒæŒ‰æ®µè½æˆ–éƒ¨åˆ†ç»„ç»‡ã€‚",
        "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
        "url": "https://example.com/project1"
    },
    ...
]

è¯·ç¡®ä¿æå–çš„ä¿¡æ¯å‡†ç¡®ä¸”ç»„ç»‡è‰¯å¥½ã€‚
"""


class Project(BaseModel):
    title: str = Field(description="The title of the project")
    description: str = Field(description="The description of the project")
    date: Optional[str] = Field(
        description="The date when the content was published or last updated"
    )
    author: Optional[str] = Field(
        description="The name of the author or organization that published the content"
    )
    content: str = Field(
        description="The main body of the content, organized into paragraphs or sections"
    )
    tags: List[str] = Field(
        description="Relevant tags or categories that describe the content"
    )
    url: str = Field(description="The URL from which the content was scraped")


class Projects(BaseModel):
    projects: List[Project]


def preprocess_result(result, urls):
    preprocessed_projects = []
    for project in result["projects"]:
        preprocessed_project = {
            "title": project.get("title", ""),
            "description": project.get("description", ""),
            "date": project.get("date", ""),
            "author": project.get("author", ""),
            "content": project.get("content", ""),
            "tags": project.get("tags", []),
            "url": project.get("url", ""),
        }
        preprocessed_projects.append(preprocessed_project)
    return {"projects": preprocessed_projects}


def run_smart_scraper_graph(prompt, url, graph_config):
    smart_scraper_graph = SmartScraperMultiGraph(
        prompt=prompt,
        source=[url],
        schema=Projects,  # è¿™é‡Œä¼ é€’çš„æ˜¯Projectsç±»
        config=graph_config,
    )
    result = smart_scraper_graph.run()
    return result


async def async_run_smart_scraper_graph(prompt, url, graph_config):
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(
        None, run_smart_scraper_graph, prompt, url, graph_config
    )
    return result


async def scrape_single_url(url, prompt, graph_config):
    try:
        result = await async_run_smart_scraper_graph(prompt, url, graph_config)
        if isinstance(result, str):
            result = json.loads(result)
        return result
    except Exception as e:
        print(f"Skipping URL due to error: {url}, Error: {str(e)}")
        return None


@app.get("/api/smart-scraper")
async def scrape(urls: str):
    urls = urls.split(",")
    if not urls:
        raise HTTPException(status_code=400, detail="No URLs provided")

    graph_config = {
        "llm": {
            "api_key": openai_key,
            "model": "gpt-4o",
            "max_tokens": 2000,
            "base_url": "https://api.gptsapi.net/v1",
        },
        "verbose": True,
        "headless": False,
        "embeddings": {
            "model": "ollama/mxbai-embed-large",
            "temperature": 0,
            "base_url": "http://localhost:11434",
        },
        "max_depth": 2,
        "max_nodes": 50,
        "verbose": True,
        "headless": False,
    }
    # graph_config = {
    #     "llm": {
    #         "api_key": openai_key,
    #         "model": "gpt-4o",
    #         "temperature": 0,
    #         "max_tokens": 2000,
    #         "base_url": "https://api.ohmygpt.com/v1",
    #     },
    #     "embeddings": {
    #         "api_key": openai_real_key,
    #         "model": "openai-text-embedding-3-small",
    #         "temperature": 0,
    #     },
    #     "verbose": True,
    #     "headless": False,
    # }

    try:
        async with Pool() as pool:
            tasks = [
                pool.apply(scrape_single_url, args=(url, prompt, graph_config))
                for url in urls
            ]
            results = await asyncio.gather(*tasks)

        # è¿‡æ»¤æ‰ None çš„ç»“æœ
        valid_results = [result for result in results if result is not None]

        # åˆå¹¶æ‰€æœ‰ç»“æœ
        combined_results = {"projects": []}
        for result in valid_results:
            combined_results["projects"].extend(result["projects"])

        # é¢„å¤„ç†ç»“æœä»¥ç¡®ä¿æ‰€æœ‰å­—æ®µæœ‰æ•ˆ
        preprocessed_result = preprocess_result(combined_results, urls)

        # Validate the result with the defined schema
        validated_result = Projects(**preprocessed_result)

        print("ğŸ§šâ€Scraping result:", validated_result)

        return JSONResponse(content=validated_result.dict())
    except Exception as e:
        print("ğŸ§šâ€Error:", str(e))
        return JSONResponse(content={"error": str(e)}, status_code=500)
